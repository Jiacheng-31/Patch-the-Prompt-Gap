# ğŸ§© RL-based Data Rewriting with VERL (GRPO) to Mitigate Catastrophic Forgetting

This repo implements an **RL-trained data rewriting agent** that rewrites downstream supervision *before* SFT to reduce distribution mismatch, stabilize training, and mitigate catastrophic forgetting âœ¨  
We use **[VERL](https://github.com/volcengine/verl)** for on-policy RL with **GRPO-style group optimization**, and train the rewriter as a lightweight **LoRA â€œpatchâ€** on top of a frozen instruction-tuned base model. Downstream SFT is done with **LLaMA-Factory** (no code changes, so it is not included here).

<p align="center">
  <img src="figure/framework.pdf" alt="Framework" width="900"/>
</p>

---

## ğŸ“ Repository Layout

```text
ACL-github/
â”œâ”€â”€ data-example/                  # small examples / data format reference
â”œâ”€â”€ figure/
â”‚   â””â”€â”€ framework.pdf              # framework figure
â”œâ”€â”€ re-writing/
â”‚   â”œâ”€â”€ rewriting_vllm.py          # apply the trained rewriter with vLLM
â”‚   â””â”€â”€ run.sh                     # rewriting entry script
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ test-general/              # general-domain retention / forgetting evaluation
â”‚   â””â”€â”€ test-math/
â”‚       â”œâ”€â”€ dataset/               # math evaluation dataset
â”‚       â”œâ”€â”€ eval_vllm.py           # vLLM-based evaluation
â”‚       â””â”€â”€ run.sh                 # math eval entry script
â””â”€â”€ verl/
    â””â”€â”€ verl/experimental/agent_loop/
        â””â”€â”€ ...                    # âœ… modified: dedicate a GPU to RewardManager
````

---

## ğŸš€ What We Do (Short Version)

* **Stage I (RL with VERL + GRPO):** train a rewriting policy (R_\phi) as a LoRA patch on a frozen base model (\pi_0).
* **Stage II (Dataset + SFT):** build a rewritten dataset using **Generateâ€“Verifyâ€“Fallback**, then run standard SFT on the rewritten data.

---

## ğŸ¯ Reward (Method-aligned)

GitHub README does not reliably render LaTeX, so we show the reward in plain text.

### Gated reward (used for GRPO)

```

r = r_task + r_task * ( lambda_dist * r_dist + lambda_div * r_div )

```

- `r_task âˆˆ {0,1}`: **hard gate** (final answer correct + reasoning valid)
- `r_dist`: **QA-style alignment** score under the frozen base model `pi0(Â·|x)` (group-normalized)
- `r_div`: **diversity** among feasible rewrites in the same group (Qwen-Embedding + marginal contribution)

**Key detail:** `r_dist` and `r_div` are computed **only when `r_task = 1`** (feasible rewrites).

---

## ğŸ§  GRPO / Group Requirement (Important)

GRPO requires sampling **K candidates per input** (same prompt `x`), and computing group statistics inside RewardManager.

* Make sure your rollout generates **K rewrites per prompt**.
* RewardManager should **evaluate rewards per group** (e.g., pass `solution_str` as `List[str]` for one prompt) so we can:

  * normalize `r_dist` within the group, and
  * compute marginal diversity `r_div`.

---

## ğŸ§© Key Implementation Notes

### âœ… Reward function changes

We mainly modify the reward to match the paper:

* hard task gate (answer + reasoning),
* QA-style alignment under `pi0(y|x)` with group normalization,
* diversity via Qwen-Embedding with marginal contribution.

### âœ… Dedicated GPU for RewardManager

We modify:

```text
verl/verl/experimental/agent_loop/
```

to allocate a dedicated GPU for the RewardManager (reduces contention and improves stability).

---

## âœï¸ Dataset Rewriting (vLLM)

Run rewriting:

```bash
cd re-writing
bash run.sh
```

Main entry:

* `re-writing/rewriting_vllm.py`

This runs **Generateâ€“Verifyâ€“Fallback**:

* generate rewrite `y~`
* if `r_task=1` accept `y~`
* else fallback to expert `y*`

---

## ğŸ§ª Evaluation

### Math evaluation

```bash
cd test/test-math
bash run.sh
```

* `test/test-math/eval_vllm.py`
* `test/test-math/dataset/`

### General retention / forgetting

* `test/test-general/`

---

## ğŸ”§ Downstream SFT (LLaMA-Factory)

We run downstream SFT with LLaMA-Factory **without modifications**, so training code/configs are not included here.

Workflow:

1. rewrite data â†’ build `D_R`
2. run SFT on `D_R` with LLaMA-Factory

---

## ğŸ“Œ Citation

```bibtex
@article{2026rlrewriting,
  title={RL-based Data Rewriting for Stable Downstream SFT},
  author={...},
  year={2026}
}
```

---

## ğŸ™ Acknowledgements

* **VERL**: [https://github.com/volcengine/verl](https://github.com/volcengine/verl)
* **vLLM** for efficient generation/evaluation
* **LLaMA-Factory** for downstream SFT (used without modification)
